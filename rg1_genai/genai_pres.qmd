---
title: "Transformers for Time Series Forecasting"
author: "Marco Zanotti"
institute: "University Milano-Bicocca"
format: 
 beamer:
  theme: Dresden
  colortheme: default
  navigation: horizontal
  header-includes: |
       \titlegraphic{\includegraphics[width=0.2\paperwidth]{img/logo-giallo.png}}
       \setbeamertemplate{page number in head/foot}[totalframenumber]
---


## Contents

1. The TSF Problem

2. Vanilla Transformer

3. TSF Transformers

4. Conclusions



# 1. The TSF Problem


##

Time series forecasting (TSF) is the task of predicting future values of a 
given sequence based on previously observed values.  

![](img/ts_for.png){fig-align="center" width="300"}


## 

The TSF problem may be essentially identified by the following aspects:  

- **Prediction objective**: $\text{\underline{point forecasting}}$ vs probabilistic forecasting  

- **Forecast horizon**: short-term vs $\text{\underline{long-term}}$ forecasting  

- **Input-Output dimension**: $\text{\underline{univariate}}$ vs multivariate forecasting  

- **Forecasting task**: $\text{\underline{single-step}}$ vs $\text{\underline{multi-step}}$ forecasting  

The TSF problem is usually faced with statistical models (ARIMA, ETS) or 
deep learning models (RNN, LSTM).  


##

The main challenges of the TSF problem are:  

- **uncertainty** increases as the forecast horizon increases  

- difficulty in capturing **multiple complex patterns** over time  

- difficulty in capturing **long-term dependencies** (critical for long-term forecasting)  

- difficulty to handle **long input sequences**    



# 2. Vanilla Transformer

##

:::: {.columns}
::: {.column width="60%"}
- Based on Encoder-Decoder architecture  
- Uses self-attention mechanism to access any part of the sequence history  
- Positional encoding allows to account for element positions  
- Residual connections and layer normalization help to stabilize the learning 
process  
- Each encoder and decoder layer is composed of a self-attention layer and a 
feed-forward layer  
:::
::: {.column width="40%"}
![](img/vanilla_arch.png){fig-align="center" height="300"}
:::
::::


## Can vanilla Transformers be used for TSF?

The TSF problem can be seen as a sequence learning problem such as machine 
translation. 
    
Main ingredients allowing to use vanilla Transformers for TSF:  

- **Multi-head Self-attention** mechanism allows to access any part of the 
sequence history, capturing both short-term and long-term dependencies 
(but it is invariant to the order of elements in a sequence)  

- **Positional encoding** allows to account for the sequence ordering  

- **Masked self-attention** allows to avoid information leakage from future  


## Can vanilla Transformers be used for TSF?

Just few changes are needed to adapt Transformers to TSF:  

- **Remove the final activation** function (softmax) from the output layer and set 
the dimension of the linear layer equal to the forecasting horizon  

- **Adapt the structure** to the desired forecasting task (single-step or multi-step)  

![](img/vanilla_adjust.png){fig-align="center"}


## Problems with vanilla Transformers

- **Locally agnostic**: the attention mechanism matches queries and keys without 
considering their local context being prone to temporal anomalies  

- **Positional encoding**: only the order in which two elements occur is taken 
into account, but their temporal distance is not  

- **Computational complexity**: given a sequence of length $L$, the time and 
memory burden is $O(L^2)$, making it difficult to learn patterns in long time 
series  

- **Simple Architecture**: the architecture does not include any component
of typical importance in TSF (e.g. autocorrelation, decomposition, 
recurrent layers, etc.)



# 3. TSF Transformers


## Classification

Transformers are **very appealing for long-term TSF** due to their ability to 
learn long-range dependencies. 

Many solutions have been proposed to adapt Transformers to TSF, mainly in the 
direction to improve the encoding, adopt more efficient attention and expand 
the architecture.  

![](img/ts_transf.png){fig-align="center" width="400" height="250"}


## Informer

aa


## Informer - Architecture

![](img/informer_arch.png){fig-align="center"}


## Informer - Causal Convolution Layers

![](img/informer_arch2.png){fig-align="center"}


## Informer - ProbSparse Attention

![](img/informer_attention.png){fig-align="center"}


## Autoformer

**Autoformer** builds upon two traditional time series analysis methods:  

- **Decomposition Layer**, which allows to decompose the time series into 
seasonality and trend-cycle components, enhancing the model's ability to 
capture these components accurately  

- **Attention (Autocorrelation) Mechanism**, which replaces the standard 
self-attention used in the vanilla transformer with an autocorrelation 
mechanism, allowing to capture the temporal dependencies in the frequency
domain  


## Autoformer - Architecture

![](img/autoformer_arch.png){fig-align="center"}


## Autoformer - Decomposition Layer

Autoformer incorporates **decomposition blocks** as an inner operation of the model.  

Encoder and decoder use decomposition blocks to extract and aggregate the 
trend-cyclical and seasonal components from the series progressively, so to make
raw data easier to predict.

For an input series $X_t$ with length $L$, the decomposition layer returns 
$X_{trend}$ and $X_{seasonal}$, both of length $L$. In practice, $X_{trend}$ 
is extracted using some form of moving-average and $X_{seasonal}$ is then 
obtained by difference.  


## Autoformer - Attention Mechanism

Autoformer uses **autocorrelation within the self-attention** layer, extracting
frequency-based dependencies from $(Q,K)$. The autocorrelation block measures the 
**time-delay similarity** and aggregates the top $n$ similar sub-series to 
reduce complexity.

![](img/autoformer_attention.png){fig-align="center"}

In practice, autocorrelation of the queries and keys for all lags is calculated 
at once by Fast Fourier Transform, so to achieve $O(L logL)$ time complexity 
(similar to Informer).


<!-- ## Autoformer - Time Delay Aggregation -->

<!-- ![](img/autoformer_attention2.png){fig-align="center"} -->



# 4. Conclusions

## Conclusions

- aa

- While using various type of positional encoding can preserve some ordering 
information, it is still inevitable to have temporal information loss after
applying self-attention.   


## Bibliografy

*Ailing Z., et al., 2023, 'Are Transformers Effective for Time Series Forecasting?', AAAI*

*Haixu W., et al., 2021, 'Autoformer: Decomposition transformers with auto-correlation for long-term series forecasting', NeurIPS*

*Haoyi Z., et al., 2021, 'Informer: Beyond efficient transformer for long sequence time-series forecasting', AAAI*

*Lara-Benitez P., et al., 2021, 'Evaluation of the Transformer Architecture for Univariate Time Series Forecasting', Advances in Artificial Intelligence, CAEPIA*

*Qingsong W., et al., 2022, 'Transformers in Time Series: A Survey', AAAI*


##

\center Thank you! \center

\



# Appendix

## Autocorrelation

In theory, given a time lag $k$, autocorrelation for a single discrete variable 
$Y$ is used to measure the "relationship" (pearson correlation) between the 
variable's current value at time $t$ to its past value at time $t - k$.
$$Autocorrelation(k) = Corr(Y_t, Y_{t-k})$$

![](img/ts_autocorr.png){fig-align="center" width="250"}


## Time Series Decomposition

In time series analysis, decomposition is a method of breaking down a time 
series into three systematic components: trend-cycle, seasonal variation, and 
random fluctuations.  

![](img/ts_decomp.png){fig-align="center" height="180"}

